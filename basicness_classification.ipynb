{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Binary classification (basic/advanced) for synsets\n",
    "\n",
    "We use the already classified synsets from data folder as training/test set.\n",
    "For each synset provided there are one or more relevant words that belongs to the synset. And there is the label (basic/advanced) for the synset.\n",
    "\n",
    "We then consider as features for classification the following:\n",
    "1. The vector representation of the first word **in the dataset**\n",
    "2. The depth of the synset in the WordNet hierarchy\n",
    "3. The pronunciation complexity of the first word **in the dataset**\n",
    "4. The length of the first word **in the dataset**\n",
    "5. The synset classification to concrete or abstract concept\n",
    "\n",
    "Given a new synset we want to classify we then get its features by:\n",
    "1. The vector representation of the first word **in the synset**\n",
    "2. Getting the depth of the sysnet in the WordNet hierarchy\n",
    "3. Getting the pronunciation complexity of the first (most frequently used) word **in the synset**\n",
    "4. Getting the length of the first (most frequently used) word **in the synset**\n",
    "5. Predicting the synset classification to concrete or abstract concept\n",
    "\n",
    "After defining data we train a binary classifier to predict the label (basic/advanced) of the synset.\n",
    "Since we have a small dataset we use a simple logistic regression classifier trained using 5-fold cross validation.\n",
    "\n",
    "Steps:\n",
    "1. Load and format the JSON dataset (synsets, word(s), labels, definitions) "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee2a4e9d05762d02"
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "import json\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import openpyxl # install it as it is required by pandas to read excel files\n",
    "\n",
    "from nltk.corpus.reader import Synset\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import cmudict\n",
    "import spacy\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "# nltk.download('wordnet')\n",
    "# # download the CMU Pronouncing Dictionary\n",
    "# nltk.download('cmudict')\n",
    "# spacy.cli.download(\"en_core_web_md\")\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T09:52:51.594762900Z",
     "start_time": "2024-04-17T09:52:50.397713100Z"
    }
   },
   "id": "1bd01b4e9d9962d4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Load and format the JSON dataset (synsets, word(s), labels, definitions)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6466c50de33d51bf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.1 Functions needed later on"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b17c7e3f3f9c3bb2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get WordNet Synset from string of the form \"Synset('word.pos.n')\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6226de7c56eca0db"
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "outputs": [],
   "source": [
    "def get_synset_from_string(s: str) -> Synset:\n",
    "    # find first ' and last '\n",
    "    start = s.find('\\'')\n",
    "    end = s.rfind('\\'')\n",
    "    # get synset name from start to end\n",
    "    synset_name = s[start + 1:end]\n",
    "    return wn.synset(synset_name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T09:52:51.610712Z",
     "start_time": "2024-04-17T09:52:51.596729600Z"
    }
   },
   "id": "d47500d668011346"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Calculate pronunce complexity based on number of phonemes\n",
    "If word not found in dictionary return 50, indicating high complexity (max complexity is around 25 in this dataset)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c931e622e1aeaf0d"
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "outputs": [],
   "source": [
    "pronunce_dict = cmudict.dict()\n",
    "\n",
    "def calculate_pronunce_complexity(sentence: str) -> int:\n",
    "    sentence = sentence.split()\n",
    "    complexity = 0\n",
    "    for word in sentence:\n",
    "        if word.lower() in pronunce_dict:\n",
    "            phonemes = pronunce_dict[word.lower()][0]  # get phonetic representation\n",
    "            complexity += len(phonemes)  # complexity based on number of phonemes\n",
    "        else:\n",
    "            return 50 # if a word is not found in dictionary return high complexity since it is not a common word\n",
    "    \n",
    "    if complexity == 0: # if no word is found in dictionary return high complexity since there are no common words\n",
    "        return 50\n",
    "    return complexity"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T09:52:52.229712500Z",
     "start_time": "2024-04-17T09:52:51.611713100Z"
    }
   },
   "id": "f4e1feb1b71871ea"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2 Load dataset and concrete/abstract classifier"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6a889f18797c2cf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load dataset from JSON file and get dataset and answers values"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e5ce0b6a94225e4"
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "outputs": [],
   "source": [
    "with open('data/basicness_dataset.json') as f:\n",
    "    data = json.load(f)\n",
    "    dataset = data['dataset']\n",
    "    labels = data['answers']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T09:52:52.246712100Z",
     "start_time": "2024-04-17T09:52:52.231712Z"
    }
   },
   "id": "6504e45c9fc1ff6d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load Logistic Regression classifier used later to predict if a word is abstract or concrete"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "224cef1397d7fc09"
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "outputs": [],
   "source": [
    "# get Logistic Regression classifier with joblib\n",
    "concrete_abstract_cls: LogisticRegression = joblib.load('trained_models/concrete_abstract_classifier.joblib')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T09:52:52.275753300Z",
     "start_time": "2024-04-17T09:52:52.245714300Z"
    }
   },
   "id": "96d9c7c43e15cee7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3 Create dataset containing the features"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c8dcf3cab1afce09"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a new DataFrame containing the original dataset information\n",
    "- Synset\n",
    "- Words\n",
    "- Label\n",
    "- Definition\n",
    "\n",
    "plus the features needed for training and prediction:\n",
    "- Synset depth\n",
    "- Pronunciation complexity\n",
    "- Length of most frequently used word\n",
    "- Abstract/concrete classification\n",
    "- Vector representation of most frequently used word"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "edf80f28f815e534"
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "outputs": [],
   "source": [
    "cols = ['synset', 'words', 'synset_depth', 'pronunce_complexity', 'first_word_length', 'abstract', 'word_vector', 'label', 'definition']\n",
    "dataset_df: DataFrame = pd.DataFrame(columns=cols)\n",
    "\n",
    "splitted_list: List[List[str]] = []\n",
    "label_index = 0\n",
    "for row in dataset:\n",
    "    # split elements in original dataset string\n",
    "    row_list = []\n",
    "    temp_split = row.split(':')\n",
    "    for elem in temp_split:\n",
    "        splitted = elem.split('|')\n",
    "        row_list.extend([x for x in splitted])\n",
    "    \n",
    "    # get synset\n",
    "    synset: Synset = get_synset_from_string(row_list[0])\n",
    "    # get words\n",
    "    words = row_list[1]\n",
    "    words = words.split(',')\n",
    "    words = [word.strip() for word in words]\n",
    "    # take only first word, it should be the most frequently used\n",
    "    first_word = words[0]\n",
    "    # get synset depth\n",
    "    synset_depth = synset.max_depth()\n",
    "    # get pronunce complexity\n",
    "    pronunce = calculate_pronunce_complexity(first_word)\n",
    "    # get first word length\n",
    "    first_word_length = len(first_word)\n",
    "    # get concreteness\n",
    "    word_vector = nlp(first_word)[0].vector\n",
    "    # alternative using probability instead of binary label: \n",
    "    #   need to change the classifier for considering this as a numerical features instead of categorical\n",
    "    # is_abstract = concrete_abstract_cls.predict_proba([word_vector])[0][1]\n",
    "    is_abstract = concrete_abstract_cls.predict([word_vector])[0]\n",
    "    # get label\n",
    "    label = labels[label_index]\n",
    "    label_index += 1\n",
    "    # get definition\n",
    "    definition = row_list[3]\n",
    "    # add row to dataframe\n",
    "    new_row = [[synset, first_word, synset_depth, pronunce, first_word_length, is_abstract, word_vector, label, definition]]\n",
    "    dataset_df = pd.concat(\n",
    "        [dataset_df, pd.DataFrame(new_row, columns=cols)], \n",
    "        ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T09:52:54.464740500Z",
     "start_time": "2024-04-17T09:52:52.268712200Z"
    }
   },
   "id": "1bd456c7d41782ce"
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "outputs": [
    {
     "data": {
      "text/plain": "                                        synset                       words  \\\n0                           Synset('war.n.01')                         war   \n1                       Synset('fiefdom.n.01')                     fiefdom   \n2                           Synset('bed.n.03')                         bed   \n3    Synset('return_on_invested_capital.n.01')  return on invested capital   \n4                       Synset('texture.n.02')                     texture   \n..                                         ...                         ...   \n499                     Synset('reading.n.03')                     reading   \n500           Synset('sanctimoniousness.n.01')           sanctimoniousness   \n501                  Synset('chalcedony.n.01')                  chalcedony   \n502                    Synset('stopcock.n.01')                    stopcock   \n503                  Synset('backpacker.n.01')                  backpacker   \n\n    synset_depth pronunce_complexity first_word_length abstract  \\\n0              7                   3                 3        1   \n1              6                   6                 7        1   \n2              5                   3                 3        0   \n3              6                  22                26        1   \n4              9                   6                 7        0   \n..           ...                 ...               ...      ...   \n499            6                   5                 7        0   \n500           10                  50                17        1   \n501            8                   9                10        0   \n502           11                  50                 8        0   \n503           10                  50                10        0   \n\n                                           word_vector     label  \\\n0    [1.4858, -1.8245, -3.4561, -2.0548, 4.5762, 3....     basic   \n1    [-4.6732, -7.3621, 0.26127, 2.5247, 4.8547, -5...  advanced   \n2    [-2.0862, 1.5808, -7.5852, -1.8082, -1.3864, 3...     basic   \n3    [-1.72, 1.7105, -1.5638, 1.3427, 4.4956, 5.316...  advanced   \n4    [-1.7606, -0.68817, -2.7257, 0.86493, -0.88825...     basic   \n..                                                 ...       ...   \n499  [1.5773, -2.6604, 1.7931, -3.062, -0.093512, -...     basic   \n500  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  advanced   \n501  [-2.3944, -0.11777, -1.3401, 3.253, 3.1655, -2...  advanced   \n502  [-2.5869, 1.5372, -2.7638, 5.6035, 1.5544, 3.7...  advanced   \n503  [-0.88119, 3.1579, -3.6337, 0.77035, -0.19718,...     basic   \n\n                                            definition  \n0        the waging of armed conflict against an enemy  \n1               the domain controlled by a feudal lord  \n2     a depression forming the ground under a body ...  \n3     (corporate finance) the amount, expressed as ...  \n4                   the essential quality of something  \n..                                                 ...  \n499   a datum about some physical state that is pre...  \n500         the quality of being hypocritically devout  \n501   a milky or greyish translucent to transparent...  \n502   faucet consisting of a rotating device for re...  \n503                       a hiker who wears a backpack  \n\n[504 rows x 9 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>synset</th>\n      <th>words</th>\n      <th>synset_depth</th>\n      <th>pronunce_complexity</th>\n      <th>first_word_length</th>\n      <th>abstract</th>\n      <th>word_vector</th>\n      <th>label</th>\n      <th>definition</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Synset('war.n.01')</td>\n      <td>war</td>\n      <td>7</td>\n      <td>3</td>\n      <td>3</td>\n      <td>1</td>\n      <td>[1.4858, -1.8245, -3.4561, -2.0548, 4.5762, 3....</td>\n      <td>basic</td>\n      <td>the waging of armed conflict against an enemy</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Synset('fiefdom.n.01')</td>\n      <td>fiefdom</td>\n      <td>6</td>\n      <td>6</td>\n      <td>7</td>\n      <td>1</td>\n      <td>[-4.6732, -7.3621, 0.26127, 2.5247, 4.8547, -5...</td>\n      <td>advanced</td>\n      <td>the domain controlled by a feudal lord</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Synset('bed.n.03')</td>\n      <td>bed</td>\n      <td>5</td>\n      <td>3</td>\n      <td>3</td>\n      <td>0</td>\n      <td>[-2.0862, 1.5808, -7.5852, -1.8082, -1.3864, 3...</td>\n      <td>basic</td>\n      <td>a depression forming the ground under a body ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Synset('return_on_invested_capital.n.01')</td>\n      <td>return on invested capital</td>\n      <td>6</td>\n      <td>22</td>\n      <td>26</td>\n      <td>1</td>\n      <td>[-1.72, 1.7105, -1.5638, 1.3427, 4.4956, 5.316...</td>\n      <td>advanced</td>\n      <td>(corporate finance) the amount, expressed as ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Synset('texture.n.02')</td>\n      <td>texture</td>\n      <td>9</td>\n      <td>6</td>\n      <td>7</td>\n      <td>0</td>\n      <td>[-1.7606, -0.68817, -2.7257, 0.86493, -0.88825...</td>\n      <td>basic</td>\n      <td>the essential quality of something</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>499</th>\n      <td>Synset('reading.n.03')</td>\n      <td>reading</td>\n      <td>6</td>\n      <td>5</td>\n      <td>7</td>\n      <td>0</td>\n      <td>[1.5773, -2.6604, 1.7931, -3.062, -0.093512, -...</td>\n      <td>basic</td>\n      <td>a datum about some physical state that is pre...</td>\n    </tr>\n    <tr>\n      <th>500</th>\n      <td>Synset('sanctimoniousness.n.01')</td>\n      <td>sanctimoniousness</td>\n      <td>10</td>\n      <td>50</td>\n      <td>17</td>\n      <td>1</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>advanced</td>\n      <td>the quality of being hypocritically devout</td>\n    </tr>\n    <tr>\n      <th>501</th>\n      <td>Synset('chalcedony.n.01')</td>\n      <td>chalcedony</td>\n      <td>8</td>\n      <td>9</td>\n      <td>10</td>\n      <td>0</td>\n      <td>[-2.3944, -0.11777, -1.3401, 3.253, 3.1655, -2...</td>\n      <td>advanced</td>\n      <td>a milky or greyish translucent to transparent...</td>\n    </tr>\n    <tr>\n      <th>502</th>\n      <td>Synset('stopcock.n.01')</td>\n      <td>stopcock</td>\n      <td>11</td>\n      <td>50</td>\n      <td>8</td>\n      <td>0</td>\n      <td>[-2.5869, 1.5372, -2.7638, 5.6035, 1.5544, 3.7...</td>\n      <td>advanced</td>\n      <td>faucet consisting of a rotating device for re...</td>\n    </tr>\n    <tr>\n      <th>503</th>\n      <td>Synset('backpacker.n.01')</td>\n      <td>backpacker</td>\n      <td>10</td>\n      <td>50</td>\n      <td>10</td>\n      <td>0</td>\n      <td>[-0.88119, 3.1579, -3.6337, 0.77035, -0.19718,...</td>\n      <td>basic</td>\n      <td>a hiker who wears a backpack</td>\n    </tr>\n  </tbody>\n</table>\n<p>504 rows × 9 columns</p>\n</div>"
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df.head(600)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T09:52:54.511712900Z",
     "start_time": "2024-04-17T09:52:54.465712200Z"
    }
   },
   "id": "36b8663e39c572e1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Train a binary classifier to predict the label (basic/advanced) of the synset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2374d432fb93d41f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1 Fix dataset formatting and split into features and labels"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eeb004511b51d831"
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "outputs": [],
   "source": [
    "# drop columns that are not features\n",
    "X = dataset_df.drop(columns=['synset', 'label', 'definition', 'words'], axis=1)\n",
    "y = dataset_df['label']\n",
    "\n",
    "# split word_vector into columns, each element of the vector is a feature (column)\n",
    "X = pd.concat([X, pd.DataFrame(X['word_vector'].to_list(), columns=[f'word_vector_{i}' for i in range(300)])], axis=1)\n",
    "X.drop(columns=['word_vector'], inplace=True) # drop original word_vector column, not needed anymore\n",
    "# X.drop(columns=['abstract'], inplace=True) # we tried to drop abstract column"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T09:52:54.589713700Z",
     "start_time": "2024-04-17T09:52:54.496712700Z"
    }
   },
   "id": "7a5b8b32ffe5e90e"
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "outputs": [
    {
     "data": {
      "text/plain": "  synset_depth pronunce_complexity first_word_length abstract  word_vector_0  \\\n0            7                   3                 3        1         1.4858   \n1            6                   6                 7        1        -4.6732   \n2            5                   3                 3        0        -2.0862   \n3            6                  22                26        1        -1.7200   \n4            9                   6                 7        0        -1.7606   \n\n   word_vector_1  word_vector_2  word_vector_3  word_vector_4  word_vector_5  \\\n0       -1.82450       -3.45610       -2.05480        4.57620         3.0929   \n1       -7.36210        0.26127        2.52470        4.85470        -5.0618   \n2        1.58080       -7.58520       -1.80820       -1.38640         3.3168   \n3        1.71050       -1.56380        1.34270        4.49560         5.3168   \n4       -0.68817       -2.72570        0.86493       -0.88825        -6.8168   \n\n   ...  word_vector_290  word_vector_291  word_vector_292  word_vector_293  \\\n0  ...          11.6350         -3.57470          0.10567          6.78690   \n1  ...           5.6480          0.22874          3.14500          2.24750   \n2  ...           3.0212         -2.85940          3.45250          0.70655   \n3  ...           4.1487         -0.13711         -3.02250          1.78690   \n4  ...           2.2933          0.69526          4.43730         -2.30090   \n\n   word_vector_294  word_vector_295  word_vector_296  word_vector_297  \\\n0          -3.8354          2.26210         -0.92491         -0.51409   \n1           5.1050          5.31620         -3.21550         -3.52130   \n2          -8.1775         -0.32947         -5.41470          2.30300   \n3           1.6244          1.41620         -2.02410         -2.73480   \n4          -1.0168         -0.34995          5.30900         -0.48802   \n\n   word_vector_298  word_vector_299  \n0          -5.9212         -0.30886  \n1           1.1198          0.96926  \n2          -1.9646          1.64480  \n3          -4.6322          0.12388  \n4          -2.6492          0.15630  \n\n[5 rows x 304 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>synset_depth</th>\n      <th>pronunce_complexity</th>\n      <th>first_word_length</th>\n      <th>abstract</th>\n      <th>word_vector_0</th>\n      <th>word_vector_1</th>\n      <th>word_vector_2</th>\n      <th>word_vector_3</th>\n      <th>word_vector_4</th>\n      <th>word_vector_5</th>\n      <th>...</th>\n      <th>word_vector_290</th>\n      <th>word_vector_291</th>\n      <th>word_vector_292</th>\n      <th>word_vector_293</th>\n      <th>word_vector_294</th>\n      <th>word_vector_295</th>\n      <th>word_vector_296</th>\n      <th>word_vector_297</th>\n      <th>word_vector_298</th>\n      <th>word_vector_299</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7</td>\n      <td>3</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1.4858</td>\n      <td>-1.82450</td>\n      <td>-3.45610</td>\n      <td>-2.05480</td>\n      <td>4.57620</td>\n      <td>3.0929</td>\n      <td>...</td>\n      <td>11.6350</td>\n      <td>-3.57470</td>\n      <td>0.10567</td>\n      <td>6.78690</td>\n      <td>-3.8354</td>\n      <td>2.26210</td>\n      <td>-0.92491</td>\n      <td>-0.51409</td>\n      <td>-5.9212</td>\n      <td>-0.30886</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6</td>\n      <td>6</td>\n      <td>7</td>\n      <td>1</td>\n      <td>-4.6732</td>\n      <td>-7.36210</td>\n      <td>0.26127</td>\n      <td>2.52470</td>\n      <td>4.85470</td>\n      <td>-5.0618</td>\n      <td>...</td>\n      <td>5.6480</td>\n      <td>0.22874</td>\n      <td>3.14500</td>\n      <td>2.24750</td>\n      <td>5.1050</td>\n      <td>5.31620</td>\n      <td>-3.21550</td>\n      <td>-3.52130</td>\n      <td>1.1198</td>\n      <td>0.96926</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>3</td>\n      <td>3</td>\n      <td>0</td>\n      <td>-2.0862</td>\n      <td>1.58080</td>\n      <td>-7.58520</td>\n      <td>-1.80820</td>\n      <td>-1.38640</td>\n      <td>3.3168</td>\n      <td>...</td>\n      <td>3.0212</td>\n      <td>-2.85940</td>\n      <td>3.45250</td>\n      <td>0.70655</td>\n      <td>-8.1775</td>\n      <td>-0.32947</td>\n      <td>-5.41470</td>\n      <td>2.30300</td>\n      <td>-1.9646</td>\n      <td>1.64480</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>22</td>\n      <td>26</td>\n      <td>1</td>\n      <td>-1.7200</td>\n      <td>1.71050</td>\n      <td>-1.56380</td>\n      <td>1.34270</td>\n      <td>4.49560</td>\n      <td>5.3168</td>\n      <td>...</td>\n      <td>4.1487</td>\n      <td>-0.13711</td>\n      <td>-3.02250</td>\n      <td>1.78690</td>\n      <td>1.6244</td>\n      <td>1.41620</td>\n      <td>-2.02410</td>\n      <td>-2.73480</td>\n      <td>-4.6322</td>\n      <td>0.12388</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>9</td>\n      <td>6</td>\n      <td>7</td>\n      <td>0</td>\n      <td>-1.7606</td>\n      <td>-0.68817</td>\n      <td>-2.72570</td>\n      <td>0.86493</td>\n      <td>-0.88825</td>\n      <td>-6.8168</td>\n      <td>...</td>\n      <td>2.2933</td>\n      <td>0.69526</td>\n      <td>4.43730</td>\n      <td>-2.30090</td>\n      <td>-1.0168</td>\n      <td>-0.34995</td>\n      <td>5.30900</td>\n      <td>-0.48802</td>\n      <td>-2.6492</td>\n      <td>0.15630</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 304 columns</p>\n</div>"
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T09:52:54.593711700Z",
     "start_time": "2024-04-17T09:52:54.543712100Z"
    }
   },
   "id": "c838114bcf950edc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 Split data into training and test set, preprocess and create pipeline"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3f847b53e6b93ae"
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# define numerical features\n",
    "word_vector_features = [f'word_vector_{i}' for i in range(300)] # get column names for word_vector features\n",
    "numeric_features = ['synset_depth', 'pronunce_complexity', 'first_word_length'] + word_vector_features\n",
    "\n",
    "# create feature transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features)\n",
    "    ])\n",
    "\n",
    "# create pipeline for preprocessing and classification\n",
    "clf = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression()),\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T09:52:54.594712Z",
     "start_time": "2024-04-17T09:52:54.577724400Z"
    }
   },
   "id": "67680b3fe945eb97"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3 Train and evaluate classifier"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e602cf0e3a35aed"
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.7722772277227723\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    advanced       0.68      0.69      0.68        36\n",
      "       basic       0.83      0.82      0.82        65\n",
      "\n",
      "    accuracy                           0.77       101\n",
      "   macro avg       0.75      0.75      0.75       101\n",
      "weighted avg       0.77      0.77      0.77       101\n"
     ]
    }
   ],
   "source": [
    "# train classifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# predict test set and evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "print(f\"Accuracy score: {accuracy_score(y_test, y_pred)}\")\n",
    "print(classification_report(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T09:52:54.715711300Z",
     "start_time": "2024-04-17T09:52:54.596712300Z"
    }
   },
   "id": "61e116a442ad0ea5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "5-fold cross validation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fd197c76baa3e710"
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 0.7420990099009902\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "print(f\"Mean accuracy: {scores.mean()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T09:52:54.906712300Z",
     "start_time": "2024-04-17T09:52:54.653712700Z"
    }
   },
   "id": "2d8b3088ca3b66b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Additional code"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "24659f35eb06ce8c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1 Correlation between features and labels (basic/advanced)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6357939636177c"
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "def chi_square_test(feature, label):\n",
    "    contingency_table = pd.crosstab(feature, label)\n",
    "    chi2, p_val, _, _ = chi2_contingency(contingency_table)\n",
    "    return chi2, p_val"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T09:52:54.919712700Z",
     "start_time": "2024-04-17T09:52:54.905712600Z"
    }
   },
   "id": "18a7f60d5b2c1a68"
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "outputs": [],
   "source": [
    "chi_square_results = {}\n",
    "\n",
    "labels = dataset_df['label']\n",
    "# convert label to binary\n",
    "binary_labels = labels.apply(lambda x: 1 if x == 'advanced' else 0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T09:52:54.965739100Z",
     "start_time": "2024-04-17T09:52:54.920928800Z"
    }
   },
   "id": "967d36ae96948a68"
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between synset_depth and labels\n",
      "\tPearson: 0.3092353073716985\n",
      "\tSpearman: 0.3085555402867728\n",
      "\tChi-square test: 52.5260479799988, p-value: 2.188834326325188e-07\n",
      "\n",
      "Correlation between pronunce_complexity and labels\n",
      "\tPearson: 0.4667051196561559\n",
      "\tSpearman: 0.5158882133823424\n",
      "\tChi-square test: 151.45632548090128, p-value: 3.3062077113394753e-22\n",
      "\n",
      "Correlation between first_word_length and labels\n",
      "\tPearson: 0.4435413690068497\n",
      "\tSpearman: 0.4488886147922671\n",
      "\tChi-square test: 113.39734295225205, p-value: 1.1443724223891585e-14\n",
      "\n",
      "Correlation between abstract and labels\n",
      "\tPearson: 0.058893876665495996\n",
      "\tSpearman: 0.0588938766654962\n",
      "\tChi-square test: 1.5176244751259842, p-value: 0.2179793352116985\n"
     ]
    }
   ],
   "source": [
    "for col in dataset_df.columns:\n",
    "    if col in ['synset', 'words', 'label', 'definition', 'word_vector']:\n",
    "        continue\n",
    "    pearson_corr = dataset_df[col].corr(binary_labels)\n",
    "    spearman_corr = dataset_df[col].corr(binary_labels, 'spearman')\n",
    "    chi2, p_val = chi_square_test(dataset_df[col], binary_labels)\n",
    "    chi_square_results[col] = {'Chi-square': chi2, 'p-value': p_val}\n",
    "    print(f\"Correlation between {col} and labels\")\n",
    "    print(f\"\\tPearson: {pearson_corr}\")\n",
    "    print(f\"\\tSpearman: {spearman_corr}\")\n",
    "    print(f\"\\tChi-square test: {chi2}, p-value: {p_val}\")\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T09:52:54.976715800Z",
     "start_time": "2024-04-17T09:52:54.936712Z"
    }
   },
   "id": "79a435a25935d2cb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2 Predict basicness of a new synset or word"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c59a733114a9fcce"
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "outputs": [],
   "source": [
    "def predict_basicness(to_predict: Synset | str) -> str:\n",
    "    if isinstance(to_predict, str):\n",
    "        # get first synset\n",
    "        synsets = wn.synsets(to_predict)\n",
    "        if len(synsets) == 0:\n",
    "            print(f\"Warning: word '{to_predict}' not found in WordNet\")\n",
    "            return \"advanced\" # we consider a word not related to any concept in WordNet as advanced\n",
    "        synset = synsets[0]\n",
    "        print(\"Synset selected:\", synset)\n",
    "    elif isinstance(to_predict, Synset):\n",
    "        synset = to_predict\n",
    "    else:\n",
    "        raise ValueError(\"to_predict must be a string or a Synset\")\n",
    "    \n",
    "    # get words\n",
    "    words = synset.lemma_names()\n",
    "    words = [word.strip() for word in words]\n",
    "    # take only first word, it should be the most frequently used\n",
    "    first_word = words[0]\n",
    "    # get synset depth\n",
    "    synset_depth = synset.max_depth()\n",
    "    # get pronunce complexity\n",
    "    pronunce = calculate_pronunce_complexity(first_word)\n",
    "    # get first word length\n",
    "    first_word_length = len(first_word)\n",
    "    # get concreteness\n",
    "    word_vector = nlp(first_word)[0].vector\n",
    "    is_abstract = concrete_abstract_cls.predict([word_vector])[0]\n",
    "    \n",
    "    X = pd.DataFrame([[synset_depth, pronunce, first_word_length, is_abstract, *word_vector]], columns=['synset_depth', 'pronunce_complexity', 'first_word_length', 'abstract', *word_vector_features])\n",
    "    # predict\n",
    "    return clf.predict(X)[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T09:52:54.989728700Z",
     "start_time": "2024-04-17T09:52:54.970711800Z"
    }
   },
   "id": "7478ae6941265557"
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Synset('dog.n.01')\n",
      "Result: basic \n",
      "\n",
      "Input: person\n",
      "Synset selected: Synset('person.n.01')\n",
      "Result: basic\n",
      "\n",
      "Input: car\n",
      "Synset selected: Synset('car.n.01')\n",
      "Result: basic\n",
      "\n",
      "Input: apple\n",
      "Synset selected: Synset('apple.n.01')\n",
      "Result: basic\n",
      "\n",
      "Input: tree\n",
      "Synset selected: Synset('tree.n.01')\n",
      "Result: basic\n",
      "\n",
      "Input: galaxy\n",
      "Synset selected: Synset('galaxy.n.01')\n",
      "Result: advanced\n",
      "\n",
      "Input: rpg\n",
      "Warning: word 'rpg' not found in WordNet\n",
      "Result: advanced\n",
      "\n",
      "Input: celebrity\n",
      "Synset selected: Synset('celebrity.n.01')\n",
      "Result: advanced\n",
      "\n",
      "Input: aberration\n",
      "Synset selected: Synset('aberrance.n.01')\n",
      "Result: advanced\n",
      "\n",
      "Input: fps\n",
      "Synset selected: Synset('federal_protective_service.n.01')\n",
      "Result: advanced\n"
     ]
    }
   ],
   "source": [
    "# test prediction\n",
    "synset = wn.synset('dog.n.01')\n",
    "print(f\"Input: {synset}\")\n",
    "print(f\"Result: {predict_basicness(synset)} \\n\")\n",
    "\n",
    "words = ['person', 'car', 'apple', 'tree', 'galaxy', 'rpg', 'celebrity', 'aberration', 'fps']\n",
    "\n",
    "for word in words:\n",
    "    print(f\"Input: {word}\")\n",
    "    print(f\"Result: {predict_basicness(word)}\\n\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T09:52:55.111712900Z",
     "start_time": "2024-04-17T09:52:54.985712700Z"
    }
   },
   "id": "edd9aeb01e7d9e20"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
