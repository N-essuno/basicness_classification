{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Binary classification (basic/advanced) for synsets\n",
    "\n",
    "We use the already classified synsets from data folder as training/test set.\n",
    "For each synset provided there are one or more relevant words that belongs to the synset. And there is the label (basic/advanced) for the synset.\n",
    "\n",
    "We then consider as features for classification the following:\n",
    "1. The depth of the synset in the WordNet hierarchy\n",
    "2. The pronunciation complexity of the first word **in the dataset**\n",
    "3. The length of the first word **in the dataset**\n",
    "4. The synset classification to concrete or abstract concept\n",
    "\n",
    "Given a new synset we want to classify we then get its features by:\n",
    "1. Getting the depth of the sysnet in the WordNet hierarchy\n",
    "2. Getting the pronunciation complexity of the first (most frequently used) word **in the synset**\n",
    "3. Getting the length of the first (most frequently used) word **in the synset**\n",
    "4. Predicting the synset classification to concrete or abstract concept\n",
    "\n",
    "After defining data we train a binary classifier to predict the label (basic/advanced) of the synset.\n",
    "Since we have a small dataset we use a simple logistic regression classifier trained using 5-fold cross validation.\n",
    "\n",
    "Steps:\n",
    "1. Load and format the JSON dataset (synsets, word(s), labels, definitions) "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee2a4e9d05762d02"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "import json\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import openpyxl # install it as it is required by pandas to read excel files\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus.reader import Synset\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import cmudict\n",
    "import spacy\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "\n",
    "# nltk.download('wordnet')\n",
    "# # download the CMU Pronouncing Dictionary\n",
    "# nltk.download('cmudict')\n",
    "# spacy.cli.download(\"en_core_web_md\")\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T09:59:48.625147300Z",
     "start_time": "2024-04-16T09:59:37.708105Z"
    }
   },
   "id": "1bd01b4e9d9962d4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Load and format the JSON dataset (synsets, word(s), labels, definitions)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6466c50de33d51bf"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "with open('data/1.json') as f:\n",
    "    data = json.load(f)\n",
    "    dataset = data['dataset']\n",
    "    labels = data['answers']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T09:59:48.646147700Z",
     "start_time": "2024-04-16T09:59:48.628145900Z"
    }
   },
   "id": "6504e45c9fc1ff6d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get Synset from string of the form \"Synset('word.pos.n')\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6226de7c56eca0db"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def get_synset_from_string(s: str) -> Synset:\n",
    "    # find first ' and last '\n",
    "    start = s.find('\\'')\n",
    "    end = s.rfind('\\'')\n",
    "    # get synset name from start to end\n",
    "    synset_name = s[start + 1:end]\n",
    "    return wn.synset(synset_name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T09:59:48.674146500Z",
     "start_time": "2024-04-16T09:59:48.652148300Z"
    }
   },
   "id": "d47500d668011346"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Calculate pronunce complexity based on number of phonemes\n",
    "If word not found in dictionary return 50, indicating high complexity (max complexity is around 25 in this dataset)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c931e622e1aeaf0d"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "pronunce_dict = cmudict.dict()\n",
    "\n",
    "def calculate_pronunce_complexity(sentence: str) -> int:\n",
    "    sentence = sentence.split()\n",
    "    complexity = 0\n",
    "    for word in sentence:\n",
    "        if word.lower() in pronunce_dict:\n",
    "            phonemes = pronunce_dict[word.lower()][0]  # get phonetic representation\n",
    "            complexity += len(phonemes)  # complexity based on number of phonemes\n",
    "        else: # TODO check if returning high complexity if one word not found is better. Maybe\n",
    "            return 50\n",
    "    \n",
    "    if complexity == 0: # if word not found in dictionary return high complexity\n",
    "        return 50\n",
    "    return complexity"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T09:59:53.477147600Z",
     "start_time": "2024-04-16T09:59:48.680145800Z"
    }
   },
   "id": "f4e1feb1b71871ea"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Select synset and word(s) from dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "316f22dd3df6bda8"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.linear_model._logistic.LogisticRegression'>\n"
     ]
    }
   ],
   "source": [
    "# get Logistic Regression classifier with joblib\n",
    "concrete_abstract_cls: LogisticRegression = joblib.load('trained_models/concrete_abstract_classifier.joblib')\n",
    "print(type(concrete_abstract_cls))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T09:59:53.493145800Z",
     "start_time": "2024-04-16T09:59:53.482147Z"
    }
   },
   "id": "96d9c7c43e15cee7"
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "# dataframe containing synset, word(s), definition, label\n",
    "cols = ['synset', 'words', 'synset_depth', 'pronunce_complexity', 'first_word_length', 'abstract', 'word_vector', 'label', 'definition']\n",
    "dataset_df: DataFrame = pd.DataFrame(columns=cols)\n",
    "\n",
    "splitted_list: List[List[str]] = []\n",
    "label_index = 0\n",
    "for row in dataset:\n",
    "    row_list = []\n",
    "    temp_split = row.split(':')\n",
    "    for elem in temp_split:\n",
    "        splitted = elem.split('|')\n",
    "        row_list.extend([x for x in splitted])\n",
    "    \n",
    "    # get synset\n",
    "    synset: Synset = get_synset_from_string(row_list[0])\n",
    "    # get words\n",
    "    words = row_list[1]\n",
    "    words = words.split(',')\n",
    "    words = [word.strip() for word in words]\n",
    "    # take only first word for now, it should be the most frequently used\n",
    "    first_word = words[0]\n",
    "    # words = \",\".join(words)\n",
    "    # get synset depth\n",
    "    synset_depth = synset.max_depth()\n",
    "    # get pronunce complexity\n",
    "    pronunce = calculate_pronunce_complexity(first_word)\n",
    "    # get first word length\n",
    "    first_word_length = len(first_word)\n",
    "    # get concreteness\n",
    "    word_vector = list(nlp(first_word))[0].vector\n",
    "    # TODO maybe consider probability instead of binary label\n",
    "    # get if abstract or concrete, take first argument because predict returns a list in which the first element is the prediction and the second is the probability of the prediction\n",
    "    # is_abstract = concrete_abstract_cls.predict_proba([word_vector])[0][1]\n",
    "    is_abstract = concrete_abstract_cls.predict([word_vector])[0]\n",
    "    # get label\n",
    "    label = labels[label_index]\n",
    "    label_index += 1\n",
    "    # if label == 'basic':\n",
    "    #     label = 0\n",
    "    # else:\n",
    "    #     label = 1\n",
    "    # get definition\n",
    "    definition = row_list[3]\n",
    "    # add row to dataframe\n",
    "    new_row = [[synset, first_word, synset_depth, pronunce, first_word_length, is_abstract, word_vector, label, definition]]\n",
    "    dataset_df = pd.concat(\n",
    "        [dataset_df, pd.DataFrame(new_row, columns=cols)], \n",
    "        ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T11:11:47.287874300Z",
     "start_time": "2024-04-16T11:11:43.216444100Z"
    }
   },
   "id": "1bd456c7d41782ce"
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "                                        synset                       words  \\\n0                           Synset('war.n.01')                         war   \n1                       Synset('fiefdom.n.01')                     fiefdom   \n2                           Synset('bed.n.03')                         bed   \n3    Synset('return_on_invested_capital.n.01')  return on invested capital   \n4                       Synset('texture.n.02')                     texture   \n..                                         ...                         ...   \n499                     Synset('reading.n.03')                     reading   \n500           Synset('sanctimoniousness.n.01')           sanctimoniousness   \n501                  Synset('chalcedony.n.01')                  chalcedony   \n502                    Synset('stopcock.n.01')                    stopcock   \n503                  Synset('backpacker.n.01')                  backpacker   \n\n    synset_depth pronunce_complexity first_word_length abstract  \\\n0              7                   3                 3        1   \n1              6                   6                 7        1   \n2              5                   3                 3        0   \n3              6                  22                26        1   \n4              9                   6                 7        1   \n..           ...                 ...               ...      ...   \n499            6                   5                 7        1   \n500           10                  50                17        1   \n501            8                   9                10        0   \n502           11                  50                 8        1   \n503           10                  50                10        0   \n\n                                           word_vector     label  \\\n0    [1.4858, -1.8245, -3.4561, -2.0548, 4.5762, 3....     basic   \n1    [-4.6732, -7.3621, 0.26127, 2.5247, 4.8547, -5...  advanced   \n2    [-2.0862, 1.5808, -7.5852, -1.8082, -1.3864, 3...     basic   \n3    [-1.72, 1.7105, -1.5638, 1.3427, 4.4956, 5.316...  advanced   \n4    [-1.7606, -0.68817, -2.7257, 0.86493, -0.88825...     basic   \n..                                                 ...       ...   \n499  [1.5773, -2.6604, 1.7931, -3.062, -0.093512, -...     basic   \n500  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  advanced   \n501  [-2.3944, -0.11777, -1.3401, 3.253, 3.1655, -2...  advanced   \n502  [-2.5869, 1.5372, -2.7638, 5.6035, 1.5544, 3.7...  advanced   \n503  [-0.88119, 3.1579, -3.6337, 0.77035, -0.19718,...     basic   \n\n                                            definition  \n0        the waging of armed conflict against an enemy  \n1               the domain controlled by a feudal lord  \n2     a depression forming the ground under a body ...  \n3     (corporate finance) the amount, expressed as ...  \n4                   the essential quality of something  \n..                                                 ...  \n499   a datum about some physical state that is pre...  \n500         the quality of being hypocritically devout  \n501   a milky or greyish translucent to transparent...  \n502   faucet consisting of a rotating device for re...  \n503                       a hiker who wears a backpack  \n\n[504 rows x 9 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>synset</th>\n      <th>words</th>\n      <th>synset_depth</th>\n      <th>pronunce_complexity</th>\n      <th>first_word_length</th>\n      <th>abstract</th>\n      <th>word_vector</th>\n      <th>label</th>\n      <th>definition</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Synset('war.n.01')</td>\n      <td>war</td>\n      <td>7</td>\n      <td>3</td>\n      <td>3</td>\n      <td>1</td>\n      <td>[1.4858, -1.8245, -3.4561, -2.0548, 4.5762, 3....</td>\n      <td>basic</td>\n      <td>the waging of armed conflict against an enemy</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Synset('fiefdom.n.01')</td>\n      <td>fiefdom</td>\n      <td>6</td>\n      <td>6</td>\n      <td>7</td>\n      <td>1</td>\n      <td>[-4.6732, -7.3621, 0.26127, 2.5247, 4.8547, -5...</td>\n      <td>advanced</td>\n      <td>the domain controlled by a feudal lord</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Synset('bed.n.03')</td>\n      <td>bed</td>\n      <td>5</td>\n      <td>3</td>\n      <td>3</td>\n      <td>0</td>\n      <td>[-2.0862, 1.5808, -7.5852, -1.8082, -1.3864, 3...</td>\n      <td>basic</td>\n      <td>a depression forming the ground under a body ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Synset('return_on_invested_capital.n.01')</td>\n      <td>return on invested capital</td>\n      <td>6</td>\n      <td>22</td>\n      <td>26</td>\n      <td>1</td>\n      <td>[-1.72, 1.7105, -1.5638, 1.3427, 4.4956, 5.316...</td>\n      <td>advanced</td>\n      <td>(corporate finance) the amount, expressed as ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Synset('texture.n.02')</td>\n      <td>texture</td>\n      <td>9</td>\n      <td>6</td>\n      <td>7</td>\n      <td>1</td>\n      <td>[-1.7606, -0.68817, -2.7257, 0.86493, -0.88825...</td>\n      <td>basic</td>\n      <td>the essential quality of something</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>499</th>\n      <td>Synset('reading.n.03')</td>\n      <td>reading</td>\n      <td>6</td>\n      <td>5</td>\n      <td>7</td>\n      <td>1</td>\n      <td>[1.5773, -2.6604, 1.7931, -3.062, -0.093512, -...</td>\n      <td>basic</td>\n      <td>a datum about some physical state that is pre...</td>\n    </tr>\n    <tr>\n      <th>500</th>\n      <td>Synset('sanctimoniousness.n.01')</td>\n      <td>sanctimoniousness</td>\n      <td>10</td>\n      <td>50</td>\n      <td>17</td>\n      <td>1</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>advanced</td>\n      <td>the quality of being hypocritically devout</td>\n    </tr>\n    <tr>\n      <th>501</th>\n      <td>Synset('chalcedony.n.01')</td>\n      <td>chalcedony</td>\n      <td>8</td>\n      <td>9</td>\n      <td>10</td>\n      <td>0</td>\n      <td>[-2.3944, -0.11777, -1.3401, 3.253, 3.1655, -2...</td>\n      <td>advanced</td>\n      <td>a milky or greyish translucent to transparent...</td>\n    </tr>\n    <tr>\n      <th>502</th>\n      <td>Synset('stopcock.n.01')</td>\n      <td>stopcock</td>\n      <td>11</td>\n      <td>50</td>\n      <td>8</td>\n      <td>1</td>\n      <td>[-2.5869, 1.5372, -2.7638, 5.6035, 1.5544, 3.7...</td>\n      <td>advanced</td>\n      <td>faucet consisting of a rotating device for re...</td>\n    </tr>\n    <tr>\n      <th>503</th>\n      <td>Synset('backpacker.n.01')</td>\n      <td>backpacker</td>\n      <td>10</td>\n      <td>50</td>\n      <td>10</td>\n      <td>0</td>\n      <td>[-0.88119, 3.1579, -3.6337, 0.77035, -0.19718,...</td>\n      <td>basic</td>\n      <td>a hiker who wears a backpack</td>\n    </tr>\n  </tbody>\n</table>\n<p>504 rows × 9 columns</p>\n</div>"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df.head(600)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T11:11:47.863777500Z",
     "start_time": "2024-04-16T11:11:47.805768Z"
    }
   },
   "id": "36b8663e39c572e1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Measure correlation between features and label (basic/advanced)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e5a946d04d3a9a88"
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "def chi_square_test(feature, label):\n",
    "    contingency_table = pd.crosstab(feature, label)\n",
    "    chi2, p_val, _, _ = chi2_contingency(contingency_table)\n",
    "    return chi2, p_val"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T11:11:49.034047500Z",
     "start_time": "2024-04-16T11:11:48.993052800Z"
    }
   },
   "id": "18a7f60d5b2c1a68"
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'basic'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[65], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m col \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msynset\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwords\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdefinition\u001B[39m\u001B[38;5;124m'\u001B[39m]:\n\u001B[0;32m      4\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m pearson_corr \u001B[38;5;241m=\u001B[39m \u001B[43mdataset_df\u001B[49m\u001B[43m[\u001B[49m\u001B[43mcol\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcorr\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset_df\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlabel\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m spearman_corr \u001B[38;5;241m=\u001B[39m dataset_df[col]\u001B[38;5;241m.\u001B[39mcorr(dataset_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m], \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspearman\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      7\u001B[0m chi2, p_val \u001B[38;5;241m=\u001B[39m chi_square_test(dataset_df[col], dataset_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "File \u001B[1;32m~\\Desktop\\UniTo\\tln\\dicaro\\basicness_classification\\venv\\lib\\site-packages\\pandas\\core\\series.py:2974\u001B[0m, in \u001B[0;36mSeries.corr\u001B[1;34m(self, other, method, min_periods)\u001B[0m\n\u001B[0;32m   2971\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39mnan\n\u001B[0;32m   2973\u001B[0m this_values \u001B[38;5;241m=\u001B[39m this\u001B[38;5;241m.\u001B[39mto_numpy(dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mfloat\u001B[39m, na_value\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mnan, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m-> 2974\u001B[0m other_values \u001B[38;5;241m=\u001B[39m \u001B[43mother\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_numpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mfloat\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnan\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m   2976\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m method \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpearson\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspearman\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mkendall\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mcallable\u001B[39m(method):\n\u001B[0;32m   2977\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m nanops\u001B[38;5;241m.\u001B[39mnancorr(\n\u001B[0;32m   2978\u001B[0m         this_values, other_values, method\u001B[38;5;241m=\u001B[39mmethod, min_periods\u001B[38;5;241m=\u001B[39mmin_periods\n\u001B[0;32m   2979\u001B[0m     )\n",
      "File \u001B[1;32m~\\Desktop\\UniTo\\tln\\dicaro\\basicness_classification\\venv\\lib\\site-packages\\pandas\\core\\base.py:662\u001B[0m, in \u001B[0;36mIndexOpsMixin.to_numpy\u001B[1;34m(self, dtype, copy, na_value, **kwargs)\u001B[0m\n\u001B[0;32m    658\u001B[0m         values \u001B[38;5;241m=\u001B[39m values\u001B[38;5;241m.\u001B[39mcopy()\n\u001B[0;32m    660\u001B[0m     values[np\u001B[38;5;241m.\u001B[39masanyarray(isna(\u001B[38;5;28mself\u001B[39m))] \u001B[38;5;241m=\u001B[39m na_value\n\u001B[1;32m--> 662\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43masarray\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    664\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (copy \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m fillna) \u001B[38;5;129;01mor\u001B[39;00m (\u001B[38;5;129;01mnot\u001B[39;00m copy \u001B[38;5;129;01mand\u001B[39;00m using_copy_on_write()):\n\u001B[0;32m    665\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m np\u001B[38;5;241m.\u001B[39mshares_memory(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_values[:\u001B[38;5;241m2\u001B[39m], result[:\u001B[38;5;241m2\u001B[39m]):\n\u001B[0;32m    666\u001B[0m         \u001B[38;5;66;03m# Take slices to improve performance of check\u001B[39;00m\n",
      "\u001B[1;31mValueError\u001B[0m: could not convert string to float: 'basic'"
     ]
    }
   ],
   "source": [
    "chi_square_results = {}\n",
    "for col in dataset_df.columns:\n",
    "    if col in ['synset', 'words', 'label', 'definition']:\n",
    "        continue\n",
    "    pearson_corr = dataset_df[col].corr(dataset_df['label'])\n",
    "    spearman_corr = dataset_df[col].corr(dataset_df['label'], 'spearman')\n",
    "    chi2, p_val = chi_square_test(dataset_df[col], dataset_df['label'])\n",
    "    chi_square_results[col] = {'Chi-square': chi2, 'p-value': p_val}\n",
    "    print(f\"Pearson correlation between {col} and label: {pearson_corr}\")\n",
    "    print(f\"Spearman correlation between {col} and label: {spearman_corr}\")\n",
    "    print(f\"Chi-square test between {col} and label: {chi2}, p-value: {p_val}\")\n",
    "    print()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T11:11:49.492011500Z",
     "start_time": "2024-04-16T11:11:49.398913100Z"
    }
   },
   "id": "79a435a25935d2cb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Train a binary classifier to predict the label (basic/advanced) of the synset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2374d432fb93d41f"
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T11:11:50.738229700Z",
     "start_time": "2024-04-16T11:11:50.696230500Z"
    }
   },
   "id": "57d595db7ee74c1f"
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# drop columns that are not features: synset, label, definition, words, label\n",
    "X = dataset_df.drop(columns=['synset', 'label', 'definition', 'words'], axis=1)\n",
    "y = dataset_df['label']\n",
    "\n",
    "# split word_vector into columns\n",
    "X = pd.concat([X, pd.DataFrame(X['word_vector'].to_list(), columns=[f'word_vector_{i}' for i in range(300)])], axis=1)\n",
    "X.drop(columns=['word_vector'], inplace=True)\n",
    "# X.drop(columns=['abstract'], inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T11:14:17.164950300Z",
     "start_time": "2024-04-16T11:14:17.073928200Z"
    }
   },
   "id": "7a5b8b32ffe5e90e"
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(504, 303)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T11:14:17.829726600Z",
     "start_time": "2024-04-16T11:14:17.785863600Z"
    }
   },
   "id": "151048af95fdc78e"
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "data": {
      "text/plain": "  synset_depth pronunce_complexity first_word_length  word_vector_0  \\\n0            7                   3                 3         1.4858   \n1            6                   6                 7        -4.6732   \n2            5                   3                 3        -2.0862   \n3            6                  22                26        -1.7200   \n4            9                   6                 7        -1.7606   \n\n   word_vector_1  word_vector_2  word_vector_3  word_vector_4  word_vector_5  \\\n0       -1.82450       -3.45610       -2.05480        4.57620         3.0929   \n1       -7.36210        0.26127        2.52470        4.85470        -5.0618   \n2        1.58080       -7.58520       -1.80820       -1.38640         3.3168   \n3        1.71050       -1.56380        1.34270        4.49560         5.3168   \n4       -0.68817       -2.72570        0.86493       -0.88825        -6.8168   \n\n   word_vector_6  ...  word_vector_290  word_vector_291  word_vector_292  \\\n0        11.5870  ...          11.6350         -3.57470          0.10567   \n1         1.6662  ...           5.6480          0.22874          3.14500   \n2        -2.9320  ...           3.0212         -2.85940          3.45250   \n3         1.3093  ...           4.1487         -0.13711         -3.02250   \n4         3.6863  ...           2.2933          0.69526          4.43730   \n\n   word_vector_293  word_vector_294  word_vector_295  word_vector_296  \\\n0          6.78690          -3.8354          2.26210         -0.92491   \n1          2.24750           5.1050          5.31620         -3.21550   \n2          0.70655          -8.1775         -0.32947         -5.41470   \n3          1.78690           1.6244          1.41620         -2.02410   \n4         -2.30090          -1.0168         -0.34995          5.30900   \n\n   word_vector_297  word_vector_298  word_vector_299  \n0         -0.51409          -5.9212         -0.30886  \n1         -3.52130           1.1198          0.96926  \n2          2.30300          -1.9646          1.64480  \n3         -2.73480          -4.6322          0.12388  \n4         -0.48802          -2.6492          0.15630  \n\n[5 rows x 303 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>synset_depth</th>\n      <th>pronunce_complexity</th>\n      <th>first_word_length</th>\n      <th>word_vector_0</th>\n      <th>word_vector_1</th>\n      <th>word_vector_2</th>\n      <th>word_vector_3</th>\n      <th>word_vector_4</th>\n      <th>word_vector_5</th>\n      <th>word_vector_6</th>\n      <th>...</th>\n      <th>word_vector_290</th>\n      <th>word_vector_291</th>\n      <th>word_vector_292</th>\n      <th>word_vector_293</th>\n      <th>word_vector_294</th>\n      <th>word_vector_295</th>\n      <th>word_vector_296</th>\n      <th>word_vector_297</th>\n      <th>word_vector_298</th>\n      <th>word_vector_299</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7</td>\n      <td>3</td>\n      <td>3</td>\n      <td>1.4858</td>\n      <td>-1.82450</td>\n      <td>-3.45610</td>\n      <td>-2.05480</td>\n      <td>4.57620</td>\n      <td>3.0929</td>\n      <td>11.5870</td>\n      <td>...</td>\n      <td>11.6350</td>\n      <td>-3.57470</td>\n      <td>0.10567</td>\n      <td>6.78690</td>\n      <td>-3.8354</td>\n      <td>2.26210</td>\n      <td>-0.92491</td>\n      <td>-0.51409</td>\n      <td>-5.9212</td>\n      <td>-0.30886</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6</td>\n      <td>6</td>\n      <td>7</td>\n      <td>-4.6732</td>\n      <td>-7.36210</td>\n      <td>0.26127</td>\n      <td>2.52470</td>\n      <td>4.85470</td>\n      <td>-5.0618</td>\n      <td>1.6662</td>\n      <td>...</td>\n      <td>5.6480</td>\n      <td>0.22874</td>\n      <td>3.14500</td>\n      <td>2.24750</td>\n      <td>5.1050</td>\n      <td>5.31620</td>\n      <td>-3.21550</td>\n      <td>-3.52130</td>\n      <td>1.1198</td>\n      <td>0.96926</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>3</td>\n      <td>3</td>\n      <td>-2.0862</td>\n      <td>1.58080</td>\n      <td>-7.58520</td>\n      <td>-1.80820</td>\n      <td>-1.38640</td>\n      <td>3.3168</td>\n      <td>-2.9320</td>\n      <td>...</td>\n      <td>3.0212</td>\n      <td>-2.85940</td>\n      <td>3.45250</td>\n      <td>0.70655</td>\n      <td>-8.1775</td>\n      <td>-0.32947</td>\n      <td>-5.41470</td>\n      <td>2.30300</td>\n      <td>-1.9646</td>\n      <td>1.64480</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>22</td>\n      <td>26</td>\n      <td>-1.7200</td>\n      <td>1.71050</td>\n      <td>-1.56380</td>\n      <td>1.34270</td>\n      <td>4.49560</td>\n      <td>5.3168</td>\n      <td>1.3093</td>\n      <td>...</td>\n      <td>4.1487</td>\n      <td>-0.13711</td>\n      <td>-3.02250</td>\n      <td>1.78690</td>\n      <td>1.6244</td>\n      <td>1.41620</td>\n      <td>-2.02410</td>\n      <td>-2.73480</td>\n      <td>-4.6322</td>\n      <td>0.12388</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>9</td>\n      <td>6</td>\n      <td>7</td>\n      <td>-1.7606</td>\n      <td>-0.68817</td>\n      <td>-2.72570</td>\n      <td>0.86493</td>\n      <td>-0.88825</td>\n      <td>-6.8168</td>\n      <td>3.6863</td>\n      <td>...</td>\n      <td>2.2933</td>\n      <td>0.69526</td>\n      <td>4.43730</td>\n      <td>-2.30090</td>\n      <td>-1.0168</td>\n      <td>-0.34995</td>\n      <td>5.30900</td>\n      <td>-0.48802</td>\n      <td>-2.6492</td>\n      <td>0.15630</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 303 columns</p>\n</div>"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(X elems)\n",
    "X.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T11:14:18.510232600Z",
     "start_time": "2024-04-16T11:14:18.464229200Z"
    }
   },
   "id": "c838114bcf950edc"
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7722772277227723\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    advanced       0.68      0.69      0.68        36\n",
      "       basic       0.83      0.82      0.82        65\n",
      "\n",
      "    accuracy                           0.77       101\n",
      "   macro avg       0.75      0.75      0.75       101\n",
      "weighted avg       0.77      0.77      0.77       101\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define transformers\n",
    "categorical_features = ['abstract']\n",
    "word_vector_features = [f'word_vector_{i}' for i in range(300)]\n",
    "numeric_features = ['synset_depth', 'pronunce_complexity', 'first_word_length'] + word_vector_features\n",
    "\n",
    "# Create transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features)\n",
    "    ])\n",
    "\n",
    "# Create a pipeline\n",
    "clf = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('pca', PCA(n_components=0.99)),\n",
    "    ('classifier', LogisticRegression()),\n",
    "])\n",
    "\n",
    "# Fit model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T11:25:03.558174500Z",
     "start_time": "2024-04-16T11:25:03.426733600Z"
    }
   },
   "id": "61e116a442ad0ea5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "5-fold cross validation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fd197c76baa3e710"
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(clf, X, y, cv=5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T11:16:19.967430500Z",
     "start_time": "2024-04-16T11:16:19.118236200Z"
    }
   },
   "id": "b606001490ebd9d"
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 0.7420990099009902\n"
     ]
    }
   ],
   "source": [
    "# print cross validation scores and mean\n",
    "print(f\"Mean accuracy: {scores.mean()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T11:18:59.482283600Z",
     "start_time": "2024-04-16T11:18:59.457280500Z"
    }
   },
   "id": "2d8b3088ca3b66b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
